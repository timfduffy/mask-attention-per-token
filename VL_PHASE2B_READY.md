# üéâ Phase 2b Complete: Vision Token Masking!

**Status**: ‚úÖ Ready for testing
**What's New**: Mask individual vision tokens (image patches)
**Date**: Just implemented!

---

## What Phase 2b Adds

### ‚úÖ Vision Token Masking

- **Phase 2a**: Mask text tokens ‚Üí "Which text matters when image is present?"
- **Phase 2b**: Mask vision tokens ‚Üí "Which image patches matter for generation?" üÜï
- **Phase 2c**: Mask both ‚Üí Complete cross-modal analysis

### Key Capability: Spatial Analysis

Now you can answer:
- **Which image regions** matter most for text generation?
- **Which patches** trigger specific words?
- **How does** spatial location affect importance?
- **Can we create** heatmaps of critical image regions?

---

## üöÄ How to Use

### Basic Usage (Phase 2b)

```bash
# Mask vision tokens (image patches)
python mask_impact_vl.py \
  --image testimg.png \
  --prompt "What's in this image?" \
  --mask-mode vision \
  --num-tokens 3
```

**Expected Output**:
```
PHASE 2b: Image + Text (masking vision only) Masking Analysis

Loading image: testimg.png
  Image size: (128, 128)
  Vision token ranges: [(1, 17)]
  Total vision tokens: 16

Mask mode: vision
  ‚Üí Will mask 16 vision tokens (skipping 5 text tokens)

Analyzing initial prompt...
Processing layer 0/35...
  Masking vision token positions...
```

### All Three Modes

```bash
# Phase 2a: Text-only masking (default)
python mask_impact_vl.py \
  --image testimg.png \
  --prompt "What's in this image?" \
  --num-tokens 3

# Phase 2b: Vision-only masking - NEW!
python mask_impact_vl.py \
  --image testimg.png \
  --prompt "What's in this image?" \
  --mask-mode vision \
  --num-tokens 3

# Phase 2c: Mask both text and vision
python mask_impact_vl.py \
  --image testimg.png \
  --prompt "What's in this image?" \
  --mask-mode both \
  --num-tokens 1
```

### With GPU (Recommended for Vision Masking)

```bash
python mask_impact_vl.py \
  --image photo.jpg \
  --prompt "Describe this" \
  --mask-mode vision \
  --device cuda \
  --num-tokens 5
```

---

## üìä What You Can Learn

### Phase 2b Research Questions

1. **Spatial Importance**
   - Which image patches are most critical?
   - Does the center matter more than edges?
   - Are certain regions "attention sinks"?

2. **Patch-to-Text Dependencies**
   - Which patches trigger specific words?
   - Do object patches have higher impact?
   - Background vs. foreground importance?

3. **Layer-wise Vision Integration**
   - When do vision patches matter most?
   - Early layers vs. late layers?
   - Where does vision-to-text fusion happen?

4. **Per-Head Vision Specialization**
   - Do certain heads focus on specific patches?
   - Are there "vision specialist" heads?
   - Spatial attention patterns?

### Example Hypotheses

- **H1**: Center patches will show higher L2 distance (salience bias)
- **H2**: Object-containing patches matter more than background
- **H3**: Vision patch importance peaks in middle layers (15-25)
- **H4**: Some attention heads specialize in specific spatial regions

---

## üî¨ Technical Details

### Vision Token Mapping

```
128√ó128 image ‚Üí 16 vision tokens (4√ó4 spatial grid)

Patch Layout:
[1 ] [2 ] [3 ] [4 ]
[5 ] [6 ] [7 ] [8 ]
[9 ] [10] [11] [12]
[13] [14] [15] [16]

Each patch = 32√ó32 pixels (effective)
```

### Token Sequence with Vision

```
[<|vision_start|>] [patch_1] [patch_2] ... [patch_16] [<|vision_end|>] [What] ['s] [in] [this]
   Position 0         1         2     ...     16          17              18    19    20    21

Phase 2b masks: Positions 1-16 (vision patches)
Phase 2b skips: Positions 0, 17-21 (special tokens and text)
```

### Implementation

```python
# Controlled by --mask-mode flag
for mask_pos in range(current_num_tokens):
    is_vision_token = any(start <= mask_pos < end for start, end in vision_ranges)
    
    if mask_mode == 'text' and is_vision_token:
        continue  # Phase 2a: skip vision
    elif mask_mode == 'vision' and not is_vision_token:
        continue  # Phase 2b: skip text
    # mask_mode == 'both': don't skip anything
    
    # Mask this token...
```

---

## üìà Image Size ‚Üí Analysis Complexity

| Image Size | Vision Tokens | Phase 2b Runtime (GPU) | Memory |
|------------|---------------|------------------------|---------|
| 128√ó128    | 16            | ~30 sec (3 tokens)    | 11 GB   |
| 256√ó256    | 64            | ~2 min (3 tokens)     | 12 GB   |
| 512√ó512    | 256           | ~8 min (3 tokens)     | 14 GB   |
| 1024√ó1024  | 1024          | ~30 min (3 tokens)    | 18 GB   |

**Note**: Vision masking scales with number of patches!

---

## üÜö Phase Comparison Table

| Phase | Image Input | Mask Text | Mask Vision | Use Case |
|-------|-------------|-----------|-------------|----------|
| **1** | ‚ùå | ‚úÖ | N/A | Text-only baseline |
| **2a** | ‚úÖ | ‚úÖ | ‚ùå | Text context with images |
| **2b** | ‚úÖ | ‚ùå | ‚úÖ | Spatial image analysis üÜï |
| **2c** | ‚úÖ | ‚úÖ | ‚úÖ | Complete cross-modal |

---

## üí° Analysis Strategies

### 1. Start with Small Images

```bash
# Use 128√ó128 for quick tests
python mask_impact_vl.py --image small.jpg --mask-mode vision --num-tokens 1
```

### 2. Compare All Three Modes

```bash
# Run all three phases on same image
python mask_impact_vl.py --image test.jpg --prompt "What is this?" --mask-mode text --output test_2a
python mask_impact_vl.py --image test.jpg --prompt "What is this?" --mask-mode vision --output test_2b
python mask_impact_vl.py --image test.jpg --prompt "What is this?" --mask-mode both --output test_2c
```

**Load all three in visualizer to compare!**

### 3. Spatial Heatmap Creation

After running Phase 2b:
```python
# Post-process CSV to create spatial heatmaps
import pandas as pd
import numpy as np

df = pd.read_csv('output/test_2b.csv')

# Filter for specific layer and variant
layer_data = df[(df['layer'] == 20) & (df['variant'] == 'Full Layer Update')]

# Extract L2 distances for each vision token position
# Map to 4√ó4 grid for 128√ó128 image
# Visualize as heatmap!
```

### 4. Patch-Level Analysis

```bash
# Analyze which patches matter for specific generations
python mask_impact_vl.py \
  --image scene.jpg \
  --prompt "What color is the main object?" \
  --mask-mode vision \
  --num-tokens 5
```

---

## üìä Expected Results

### What Phase 2b Reveals

1. **Spatial Distribution**
   - Not all patches equally important
   - Certain regions may be "attention sinks"
   - Center vs. edge differences

2. **Object-Region Correspondence**
   - Patches containing objects: Higher L2 distance
   - Background patches: Lower impact
   - Salient features matter most

3. **Layer-Specific Patterns**
   - Early layers: Low vision patch impact
   - Middle layers (15-25): Peak vision integration
   - Late layers: Context-dependent importance

4. **Generation-Step Evolution**
   - Token 1: Initial classification (broad patches)
   - Token 2+: Detail refinement (specific patches)
   - Multi-token: Shifting spatial focus

---

## ‚úÖ Validation Checklist

Before considering Phase 2b successful:

- [ ] Run Phase 2b on 128√ó128 image
- [ ] Vision tokens get masked (check CSV output)
- [ ] Text tokens are skipped
- [ ] Output has ~16 rows per layer (for 16 patches)
- [ ] L2/cosine distances vary across patches
- [ ] Can load results in visualizer
- [ ] Compare with Phase 2a results

---

## üîç Example Use Cases

### 1. Object Localization

```bash
# Which patches contain the main object?
python mask_impact_vl.py \
  --image dog.jpg \
  --prompt "What animal is this?" \
  --mask-mode vision \
  --num-tokens 1
```

**Analysis**: Patches with dog should show high L2 distance

### 2. Background vs. Foreground

```bash
# Compare object vs. background importance
python mask_impact_vl.py \
  --image portrait.jpg \
  --prompt "Describe this person" \
  --mask-mode vision \
  --num-tokens 10
```

**Analysis**: Face patches > background patches

### 3. Multi-Object Scenes

```bash
# Which objects drive generation?
python mask_impact_vl.py \
  --image scene.jpg \
  --prompt "List the objects in this image" \
  --mask-mode vision \
  --num-tokens 20
```

**Analysis**: Track which patches matter for each generated word

### 4. Spatial Attention Patterns

```bash
# Do certain layers focus on specific regions?
python mask_impact_vl.py \
  --image landscape.jpg \
  --prompt "Describe the scenery" \
  --mask-mode vision \
  --num-tokens 15
```

**Analysis**: Early vs. late layer spatial patterns

---

## üöß Current Limitations

- ‚ùå **No automatic heatmap visualization**: CSV output only (need post-processing)
- ‚ùå **No patch-to-pixel mapping**: Tokens are abstract, not directly spatial
- ‚ùå **Large images are slow**: 1024√ó1024 = 1024 vision tokens!
- ‚ùå **No batch mode**: Single images only
- ‚ùå **No vision encoder analysis**: Decoder-level only (Phase 3)

---

## ‚è≠Ô∏è What's Next?

### Option A: Thoroughly Test Phase 2b

Experiment with:
- Different image types (photos, illustrations, diagrams)
- Different spatial layouts (centered, distributed)
- Different questions (What/Where/How many)
- Multi-token generation tracking

### Option B: Create Spatial Visualizations

Build tools to:
- Convert CSV output to heatmaps
- Map token positions to image coordinates
- Animate importance across generation steps
- Compare spatial patterns across layers

### Option C: Phase 3 (Vision Encoder)

Go deeper:
- Analyze vision encoder layers (pre-fusion)
- DeepStack feature extraction
- Patch embedding analysis
- Multi-scale vision features

### Option D: Batch Mode (Phase 2.1)

Add convenience:
- Process multiple images from YAML
- Compare across image sets
- Automated spatial analysis

---

## üìö Documentation

- **`VL_PHASE2B_READY.md`**: This guide
- **`VL_IMPLEMENTATION.md`**: Technical roadmap
- **`IMPLEMENTATION_COMPLETE.md`**: Master summary
- **`README.md`**: Project overview

---

## üéâ Success!

Phase 2b is **complete and ready**! You can now:

‚úÖ Mask individual vision tokens (patches)
‚úÖ Analyze spatial importance patterns
‚úÖ Compare text vs. vision masking
‚úÖ Track patch importance across layers
‚úÖ Identify critical image regions

**Test it now**:
```bash
python mask_impact_vl.py --image testimg.png --prompt "What is this?" --mask-mode vision --num-tokens 3
```

**Start with small images and GPU for best experience!**

Enjoy exploring spatial vision attention! üöÄüé®üîç

