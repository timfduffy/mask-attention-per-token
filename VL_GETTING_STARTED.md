# Getting Started with Qwen3-VL Phase 1

**Status**: âœ… Phase 1 Complete - Ready to use!

## What You Have Now

Phase 1 implementation for text-only masking analysis on Qwen3-VL models.

### New Files Created

| File | Purpose |
|------|---------|
| **`mask_impact_vl.py`** | Main VL analysis script (text-only) |
| **`test_vl_phase1.py`** | Validation test suite |
| **`prompts_config_vl.yaml`** | Example config (for future batch mode) |
| **`VL_IMPLEMENTATION.md`** | Technical documentation & roadmap |
| **`VL_QUICKSTART.md`** | User guide with examples |
| **`VL_PHASE1_SUMMARY.md`** | Complete implementation summary |
| **`VL_GETTING_STARTED.md`** | This file! |

## 3-Minute Quick Start

### Step 1: Install Requirements (if needed)

```bash
# Qwen3-VL requires transformers >= 4.57.0
pip install git+https://github.com/huggingface/transformers

# Verify
python -c "from transformers import Qwen3VLForConditionalGeneration; print('âœ“ Ready!')"
```

### Step 2: Run Test Suite

```bash
python test_vl_phase1.py
```

**Expected**: All 5 tests pass âœ…

### Step 3: First Analysis

```bash
python mask_impact_vl.py --prompt "What is 2+2?" --num-tokens 3
```

**Output**: 
- Console: Progress and predictions
- Files: `output/vl_masking_results.csv` and `.json`

### Step 4: Visualize

1. Open `visualize_results.html` in browser
2. Load `output/vl_masking_results.json`
3. Explore token importance across 36 layers!

## What Phase 1 Does

```
Input: "What is the capital of France?"
         â†“
[Tokenize with AutoProcessor]
         â†“
[Analyze 36 layers of Qwen3-VL]
         â†“
[For each layer, mask each token]
         â†“
[Measure impact on residual stream]
         â†“
Output: CSV/JSON with token importance data
```

**Metrics Calculated:**
- **L2 Distance**: Magnitude of change when token is masked
- **Cosine Distance**: Directional change when token is masked

**Granularity:**
- Full layer update (Attn + MLP)
- Attention-only update
- Per-head updates (32 heads)

## Comparison with Text-Only Model

| Feature | Text Model | VL Model (Phase 1) |
|---------|------------|-------------------|
| Script | `mask_impact_analysis.py` | `mask_impact_vl.py` |
| Model | Qwen3-4B | Qwen3-VL-4B |
| Layers | 28 | 36 (+29%) |
| Hidden Size | 2048 | 2560 (+25%) |
| Input | Text only | Text only (Phase 1) |
| Future | Text only | + Images (Phase 2) |
| Runtime | Baseline | ~30% slower |

**Why compare?** To validate VL architecture doesn't break text-only analysis.

## Example Workflows

### Workflow 1: Single Prompt Analysis

```bash
# Quick test
python mask_impact_vl.py \
  --prompt "The Eiffel Tower is in" \
  --num-tokens 2 \
  --device cuda

# Check output
ls output/vl_masking_results.*

# Visualize
# Open visualize_results.html â†’ Load output/vl_masking_results.json
```

### Workflow 2: Compare Models

```bash
# Same prompt, different models
PROMPT="Explain quantum computing"

# Text model
python mask_impact_analysis.py \
  --prompt "$PROMPT" \
  --num-tokens 5 \
  --output text_quantum

# VL model
python mask_impact_vl.py \
  --prompt "$PROMPT" \
  --num-tokens 5 \
  --output vl_quantum

# Load both in visualizer to compare!
```

### Workflow 3: Prompt from File

```bash
# Create prompt file
echo "What is the meaning of life?" > my_prompt.txt

# Analyze
python mask_impact_vl.py \
  --prompt my_prompt.txt \
  --num-tokens 10 \
  --output meaning_of_life
```

### Workflow 4: Development/Testing

```bash
# Minimal test (fastest)
python mask_impact_vl.py --prompt "Hi" --num-tokens 1

# Medium test
python mask_impact_vl.py --prompt "Test" --num-tokens 3

# Full analysis
python mask_impact_vl.py --prompt "Long prompt..." --num-tokens 10
```

## Understanding Output

### Console Output

```
PHASE 1: Qwen3-VL Text-Only Masking Analysis
Loading Qwen3-VL model...
âœ“ Model loaded successfully!
  Text model layers: 36
  Hidden size: 2560

Analyzing initial prompt (10 tokens)...
Pre-computing hidden states for all 36 layers...
Processing layer 0/35...
...
Processing layer 35/35...

Baseline Prediction:
  Greedy prediction: 'Paris'
  Top 5 predictions:
    1. 'Paris' (prob: 0.8234)
    2. 'France' (prob: 0.0456)
    ...

Results saved to:
  - output/vl_masking_results.csv
  - output/vl_masking_results.json
```

### Data Structure

Each row in the output represents:
```
Layer 5: When masking token "What" (position 0)
  â†’ Full update impact: L2=0.245, Cosine=0.012
  â†’ Attention impact: L2=0.123, Cosine=0.008
  â†’ Head 0 impact: L2=0.034, Cosine=0.003
  ... (32 heads total)
```

## Key Differences from Text Model

### 1. More Layers (36 vs 28)

**Implication**: Finer-grained analysis
- Early layers (0-8): Initial processing
- Middle layers (9-26): Feature extraction
- Late layers (27-35): Task-specific refinement

### 2. Larger Hidden Size (2560 vs 2048)

**Implication**: Larger absolute distances
- L2 distances ~25% higher
- More capacity for information
- Richer representations

### 3. MRoPE Instead of RoPE

**Phase 1**: Uses text section only (first 24 dims)
**Phase 2**: Will use all sections (spatial + temporal)

### 4. Model Class

**Text**: `AutoModelForCausalLM`
**VL**: `Qwen3VLForConditionalGeneration`

**Good news**: Layer access is identical! (`model.model.layers`)

## What's NOT Different (Phase 1)

âœ… Same masking logic (set attention to -inf)
âœ… Same distance metrics (L2, cosine)
âœ… Same output format (CSV/JSON)
âœ… Same visualization tool
âœ… Same early stopping mechanism

## Performance Expectations

### CPU (16 cores, 32 GB RAM)

| Prompt Tokens | Gen Tokens | Total Time |
|--------------|------------|------------|
| 10 | 1 | ~2-3 min |
| 10 | 3 | ~6-8 min |
| 10 | 5 | ~10-15 min |
| 20 | 1 | ~4-5 min |

### GPU (RTX 3090, 24 GB VRAM)

| Prompt Tokens | Gen Tokens | Total Time |
|--------------|------------|------------|
| 10 | 1 | ~15-20 sec |
| 10 | 3 | ~45-60 sec |
| 10 | 5 | ~1.5-2 min |
| 20 | 1 | ~25-30 sec |

**Note**: First run may be slower (model loading)

## Troubleshooting

### "Cannot import Qwen3VLForConditionalGeneration"

```bash
pip install --upgrade git+https://github.com/huggingface/transformers
```

### "Out of memory"

```bash
# Use CPU
python mask_impact_vl.py --device cpu ...

# Or reduce tokens
python mask_impact_vl.py --num-tokens 1 ...
```

### "Model download slow"

```bash
# Pre-download
huggingface-cli download Qwen/Qwen3-VL-4B-Instruct

# Or use local model
python mask_impact_vl.py --model /path/to/model ...
```

### "Results look strange"

1. Check test suite passes: `python test_vl_phase1.py`
2. Compare with text model on same prompt
3. Verify tokens are correctly displayed
4. Check for NaN values in output

## Next Steps After Phase 1

### Option 1: Analyze More Text Prompts

```bash
# Create a collection of interesting prompts
python mask_impact_vl.py --prompt "Prompt 1" --output exp1
python mask_impact_vl.py --prompt "Prompt 2" --output exp2
# Compare results in visualizer
```

### Option 2: Compare with Text Model

```bash
# Run same analysis on both models
# Look for:
# - Similar patterns? (validates implementation)
# - Different patterns? (interesting insights!)
```

### Option 3: Request Phase 1.1 (Batch Mode)

If you want to run many prompts:
```yaml
# prompts_config_vl.yaml already exists!
# Just need to add batch logic to mask_impact_vl.py
```

### Option 4: Proceed to Phase 2 (Images!)

Add image inputs:
```python
content = [
    {"type": "image", "image": "cat.jpg"},
    {"type": "text", "text": "What is this?"}
]
```

## Resources

- **Quick Examples**: `VL_QUICKSTART.md`
- **Technical Details**: `VL_IMPLEMENTATION.md`
- **Complete Summary**: `VL_PHASE1_SUMMARY.md`
- **Main README**: `README.md`

## Validation Checklist

Before moving to Phase 2, verify:

- [ ] Test suite passes (`python test_vl_phase1.py`)
- [ ] Can run single prompt analysis
- [ ] Output files are created in `output/`
- [ ] Visualization works correctly
- [ ] Results make intuitive sense
- [ ] Performance is acceptable for your use case

## FAQ

**Q: Why start with text-only if it's a VL model?**
A: To validate the architecture works before adding complexity.

**Q: Can I use this on CPU?**
A: Yes! Just slower (~2-3 min for 1 token vs ~20 sec on GPU).

**Q: Is the visualizer different for VL?**
A: No, same visualizer works for both text and VL models!

**Q: When will Phase 2 be ready?**
A: Depends on Phase 1 validation. Could be 1-2 weeks of development.

**Q: Can I contribute?**
A: Yes! Phase 2 and 3 are well-documented roadmaps.

---

## Ready to Start!

```bash
# 1. Test
python test_vl_phase1.py

# 2. Analyze
python mask_impact_vl.py --prompt "Your question here" --num-tokens 5

# 3. Visualize
# Open visualize_results.html â†’ Load output/vl_masking_results.json

# 4. Explore!
```

Happy analyzing! ðŸš€ðŸ”¬

